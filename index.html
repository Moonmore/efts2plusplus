
<!DOCTYPE html>
<html>
	<head>
		<title>Audio Samples</title>
		<style>
			div {
			margin-bottom: 32px;
			}
			
			.github.td {
				padding-left: 20px;
				padding-right: 20px;
			}

			
			#toc_container {
			background: #f9f9f9 none repeat scroll 0 0;
			border: 1px solid #aaa;
			display: table;
			font-size: 95%;
			padding-top: 20px;
			padding-left: 20px;
			padding-right: 20px;
			padding-bottom: 8px;
			width: auto;
			}
			
			#toc_container ul  {
			  list-style: outside none none !important; margin-left: -40px;
			}
			
			.id {
			width: 245px;
			}
			
			.ood {
			width: 280px;
			}
			
			.first-col {
			padding-right: 20px;
			white-space: nowrap;
			}
				
		  .text {
			font-style: italic;
			color: #666666;
		  }
			
		</style>
	</head>
	
	<body>
		<article>
			<header>
				<h1>Audio Samples from "EfficientTTS 2++: Fast And Lightweight Text-to-Speech Synthesis With Multi Stream Generator"</h1>
			</header>
		</article>
		
		
		<div>
			<b>Abstract: </b>Recently, the field of Text-to-speech synthesis has been predominantly characterized by end-to-end models, with the quality of speech generated by these models becoming increasingly comparable to that of human speech. In this work, we propose EfficientTTS 2++, a fast and lightweight end-to-end framework based on EfficientTTS 2 with fully differentiable. We utilize Fast Linear Attention with a Single Head (FLASH) instead of the standard stacked Transformer, which decreases computational complexity and fewer parameters. Additionally, we proposed a network structure ConvWaveNet to further decrease model parameters, and accelerated inference through a multi-stream inverse short-time Fourier Transform generator. These improvements significantly reduce model parameters and increase inference speed, thereby achieving the objectives of faster inference and lightweight modeling. Experimental results show that the proposed model achieves speech quality comparable to that of the baseline model, while also offering improved inference speed and reduced model size.
			<p></p>
		</div>
		
		<h2>Contents</h2>
			<div id="toc_container" style="padding-top:0px;">
			<ul>
				<li><a href="#Demo"> 1. Demo
				<li><a href="#Experimental"> 2. Experimental
				<li><a href="#References"> 3. References
			</ul>
		</div>
		
		<div>
			<a name="#Demo"><h2>Demo</h2></a>
			<hr>
			<table>
				<thead>
					<tr>
					<th>Ground-Truth</th>
					<th> VITS </th>
					<th> Mb-iSTFT-VITS</th>
					<th> EFTS2</th>
					<th> EFTS2++</th>
					</tr>
				  </thead>
			  <tbody>
			  <tr>
				 <td colspan="5"><span>Text 1: refuted by abundant evidence, and having no foundation whatever in truth.</span></td>
			  </tr>
			  <tr>
				<td><audio controls="" preload="none"><source src="audios/tts/gt/LJ005-0019.wav"></audio></td>
				<td><audio controls="" preload="none"><source src="audios/tts/efts/LJ005-0019.wav"></audio></td>           
				<td><audio controls="" preload="none"><source src="audios/tts/vits/id_LJ005-0019.wav"></audio></td>
				<td><audio controls="" preload="none"><source src="audios/tts/vits/id_LJ005-0019.wav"></audio></td>
				<td><audio controls="" preload="none"><source src="audios/tts/vits/id_LJ005-0019.wav"></audio></td>
			  </tr>
			  
			</tbody>
			</table>
			<hr>
		</div>

		<div>
			<p><br></p>
			
			<style>
			table {
			border-collapse: collapse;
			width: 60%; /* Adjust this value to change the width of the table */
			margin-left: auto;
			margin-right: auto;
			}
			th, td {
			border: 1px solid #ddd;
			padding: 8px;
			text-align: center;
			}
			tr:nth-child(even){background-color: #f2f2f2;}
			th {
			background-color: #3e775f;
			color: white;
			}
			</style>

			<a name="Experimental"><h2>Experimental</h2></a>
			<hr>
			<p class="lead" style="text-align: center;">Comparison of  the number of parameters, average RTF on Intel(R) Xeon(R) Gold 6130 CPU @2.10GHz and Tesla V100 GPU, and FLOPs</p>

			<table>
				<thead>
				  <tr>
					<th rowspan="2">Model</th>
					<th colspan="2">#Params (M)</th>
					<th colspan="2">RTF</th>
					<th rowspan="2">FLOPs (G)</th>
				  </tr>
				  <tr>
					<th>Total</th>
					<th>Infer</th>
					<th>CPU</th>
					<th>GPU</th>
				  </tr>
				</thead>
				<tbody>
				  <tr>
					<td>VITS</td>
					<td>35.30</td>
					<td>28.06</td>
					<td>0.575</td>
					<td>0.0159</td>
					<td>26.22</td>
				  </tr>
				  <tr>
					<td>MB-iSTFT-VITS</td>
					<td>34.67</td>
					<td>27.43</td>
					<td>0.053</td>
					<td>0.0072</td>
					<td>8.94</td>
				  </tr>
				  <tr>
					<td>EFTS2</td>
					<td>33.04</td>
					<td>24.58</td>
					<td>0.120</td>
					<td>0.0117</td>
					<td>25.84</td>
				  </tr>
				  <tr>
					<td>EFTS2++</td>
					<td>21.86</td>
					<td>13.40</td>
					<td>0.016</td>
					<td>0.0037</td>
					<td>2.65</td>
				  </tr>
				</tbody>
			</table>
		</div>

<p><br></p>
<div class="container">
<a name="References"><h2>References</h2></a>
<div>
  [1] K. Ito, "The LJ speech dataset", <a href="https://keithito.com/LJ-Speech-Dataset/">https://keithito.com/LJ-Speech-Dataset/</a>, 2017.
  <br>
  [2] J. Kim, J. Kong, J. Son, "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech," in <em>Proc. ICML</em>, 2021, pp. 5530-5540.
  <br> 
  [3] Kawamura, Masaya et al, “Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform,” in <em>ICASSP</em>, 2023. 
  <br> 
  [4] C. Miao, Q. Zhu, M. Chen et al, “Efficienttts2: Variational End-to-End Text-to-Speech Synthesis And Voice Conversion,”
  <em>IEEE ACM TASLP</em>, 2024

</div>
</div>
  
		
	</body>
</html>
